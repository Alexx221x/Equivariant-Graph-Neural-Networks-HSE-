{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6f6a5090",
      "metadata": {
        "id": "6f6a5090"
      },
      "source": [
        "# Simple Impementation of E(n) Equivariant Graph Neural Networks\n",
        "\n",
        "Original paper https://arxiv.org/pdf/2102.09844.pdf by Victor Garcia Satorras, Emiel Hoogeboom, Max Welling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bU4ixrOJCg1",
      "metadata": {
        "id": "4bU4ixrOJCg1"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cb08a10",
      "metadata": {
        "id": "8cb08a10"
      },
      "source": [
        "# Load QM9 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae30de9d",
      "metadata": {
        "id": "ae30de9d",
        "outputId": "c0b8ebf6-9600-4589-b759-5bdcfec84f27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'simple-equivariant-gnn'...\n",
            "remote: Enumerating objects: 87, done.\u001b[K\n",
            "remote: Counting objects: 100% (87/87), done.\u001b[K\n",
            "remote: Compressing objects: 100% (80/80), done.\u001b[K\n",
            "remote: Total 87 (delta 37), reused 31 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (87/87), done.\n",
            "/content/simple-equivariant-gnn\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/senya-ashukha/simple-equivariant-gnn.git\n",
        "%cd simple-equivariant-gnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "859f981c",
      "metadata": {
        "id": "859f981c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5731fade-a0e6-4a5b-c1cb-03988b84f890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys([0, 1, 6, 7, 8, 9])\n",
            "dict_keys([0, 1, 6, 7, 8, 9])\n",
            "dict_keys([0, 1, 6, 7, 8, 9])\n"
          ]
        }
      ],
      "source": [
        "# QM9 is a dataset for Molecular Property Predictions http://quantum-machine.org/datasets/\n",
        "# We will predict Highest occupied molecular orbital energy \n",
        "# https://en.wikipedia.org/wiki/HOMO_and_LUMO\n",
        "# We use data loaders from the official repo\n",
        "\n",
        "from qm9.data_utils import get_data, BatchGraph\n",
        "train_loader, val_loader, test_loader, charge_scale = get_data(num_workers=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05e20004",
      "metadata": {
        "id": "05e20004"
      },
      "source": [
        "# Graph Representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0acbcc0",
      "metadata": {
        "id": "d0acbcc0",
        "outputId": "18f6b8bf-d968-4719-d923-1b04a219cb28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "In the batch: num_graphs 96 num_nodes 1735\n",
              "> .h \t\t a tensor of nodes representations \t\tshape 1735 x 15\n",
              "> .x \t\t a tensor of nodes positions  \t\t\tshape 1735 x 3\n",
              "> .edges \t a tensor of edges, a fully connected graph \tshape 30312 x 2\n",
              "> .batch  \t a tensor of graph_ids for each node \t\ttensor([ 0,  0,  0,  ..., 95, 95, 95])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "batch = BatchGraph(iter(train_loader).next(), False, charge_scale)\n",
        "batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "784c0726",
      "metadata": {
        "id": "784c0726"
      },
      "source": [
        "# Define Equivariant Graph Convs  & GNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76e5e05f",
      "metadata": {
        "id": "76e5e05f"
      },
      "outputs": [],
      "source": [
        "def index_sum(agg_size, source, idx, cuda):\n",
        "    \"\"\"\n",
        "        source is N x hid_dim [float]\n",
        "        idx    is N           [int]\n",
        "        \n",
        "        Sums the rows source[.] with the same idx[.];\n",
        "    \"\"\"\n",
        "    tmp = torch.zeros((agg_size, source.shape[1]))\n",
        "    tmp = tmp.cuda() if cuda else tmp\n",
        "    res = torch.index_add(tmp, 0, idx, source)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d5d55db",
      "metadata": {
        "id": "4d5d55db"
      },
      "outputs": [],
      "source": [
        "def BlockLinearSiLU(input_dim, hidden_dim, **layers):\n",
        "    if layers['layer_type'] == 'f_e':\n",
        "        return nn.Sequential(\n",
        "                            nn.Linear(1 + 2 * input_dim, hidden_dim), nn.SiLU(),\n",
        "                            nn.Linear(hidden_dim, hidden_dim), nn.SiLU()\n",
        "                            )\n",
        "    elif  layers['layer_type'] == 'f_h':\n",
        "        return nn.Sequential(\n",
        "                            nn.Linear(hidden_dim + input_dim, hidden_dim), nn.SiLU(),\n",
        "                            nn.Linear(hidden_dim, hidden_dim)\n",
        "                            )\n",
        "\n",
        "\n",
        "def BlockLinearSigmoid(hidden_dim): \n",
        "    return nn.Sequential( \n",
        "                        nn.Linear(hidden_dim, 1),\n",
        "                        nn.Sigmoid()\n",
        "                        ) \n",
        "\n",
        "  \n",
        "class ConvEGNN(nn.Module):\n",
        "    def __init__(self, in_dim, hid_dim, cuda=True):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.cuda = cuda\n",
        "        \n",
        "        # computes messages based on hidden representations -> [0, 1]\n",
        "        self.f_e = BlockLinearSiLU(in_dim, hid_dim, layer_type='f_e')\n",
        "\n",
        "        # preducts \"soft\" edges based on messages \n",
        "        self.f_inf = BlockLinearSigmoid(hid_dim)\n",
        "        \n",
        "        # updates hidden representations -> [0, 1]\n",
        "        self.f_h = BlockLinearSiLU(in_dim, hid_dim, layer_type='f_h')\n",
        "\n",
        "    \n",
        "    def forward(self, b):\n",
        "        # compute distances for all edges\n",
        "        e_st, e_end = b.edges[:, 0], b.edges[:, 1]\n",
        "        dists = torch.norm(b.x[e_st] - b.x[e_end], dim=1).reshape(-1, 1)\n",
        "\n",
        "        # compute messages\n",
        "        tmp = torch.hstack([b.h[e_st], b.h[e_end], dists])\n",
        "        m_ij = self.f_e(tmp)\n",
        "\n",
        "        # predict edges\n",
        "        e_ij = self.f_inf(m_ij)\n",
        "        \n",
        "        # average e_ij-weighted messages  \n",
        "        # m_i is num_nodes x hid_dim\n",
        "        m_i = index_sum(b.h.shape[0], e_ij * m_ij, b.edges[:,0], self.cuda)\n",
        "        \n",
        "        # update hidden representations\n",
        "        b.h = b.h + self.f_h(torch.hstack([b.h, m_i]))\n",
        "        # see appendix C. Implementatation details\n",
        "\n",
        "        return b\n",
        "\n",
        "\n",
        "class NetEGNN(nn.Module):\n",
        "    def __init__(self, in_dim=15, hid_dim=128, out_dim=1, n_layers=7, cuda=True):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        self.emb = nn.Linear(in_dim, hid_dim) \n",
        "\n",
        "        # Make gnn of n_layers\n",
        "        self.gnn = nn.Sequential(*[ConvEGNN(hid_dim, hid_dim, cuda=cuda) for _ in range(n_layers)])\n",
        "\n",
        "        self.pre_mlp = nn.Sequential(\n",
        "            nn.Linear(hid_dim, hid_dim), nn.SiLU(),\n",
        "            nn.Linear(hid_dim, hid_dim))\n",
        "        \n",
        "        self.post_mlp = nn.Sequential(\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(hid_dim, hid_dim), nn.SiLU(),\n",
        "            nn.Linear(hid_dim, out_dim))\n",
        "\n",
        "        if cuda: self.cuda()\n",
        "        self.cuda = cuda\n",
        "    \n",
        "    \n",
        "    def forward(self, b):\n",
        "        b.h = self.emb(b.h)\n",
        "        \n",
        "        b = self.gnn(b)\n",
        "        h_nodes = self.pre_mlp(b.h)\n",
        "        \n",
        "        # h_graph is num_graphs x hid_dim\n",
        "        h_graph = index_sum(b.nG, h_nodes, b.batch, self.cuda) \n",
        "        \n",
        "        out = self.post_mlp(h_graph)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7f4cef6",
      "metadata": {
        "id": "b7f4cef6"
      },
      "outputs": [],
      "source": [
        "epochs = 1000\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "\n",
        "model = NetEGNN(n_layers=7, cuda=cuda) # Оптимальное ли количество?\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-16)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, verbose=False)\n",
        "\n",
        "### TEST ###\n",
        "# optimizer =  torch.optim.Adadelta(model.parameters(), lr=1e-3)\n",
        "# lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "#     factor=0.1, patience=10, threshold=0.0001, threshold_mode='abs')\n",
        "# ignite.handlers.EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "I5QukbbMlDv1"
      },
      "id": "I5QukbbMlDv1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4e5d6b1c",
      "metadata": {
        "id": "4e5d6b1c"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de3613c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de3613c9",
        "outputId": "4f84a320-0d19-4021-f82b-6c2ae9ef0252"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> start training\n",
            "> epoch 000: train 371.102 val 288.344 test 286.007 (90.6 sec)\n",
            "> epoch 001: train 271.244 val 263.889 test 262.990 (88.8 sec)\n",
            "> epoch 002: train 217.452 val 188.507 test 187.269 (89.0 sec)\n",
            "> epoch 003: train 188.802 val 167.326 test 166.806 (88.9 sec)\n",
            "> epoch 004: train 172.135 val 161.350 test 161.055 (88.8 sec)\n",
            "> epoch 005: train 155.915 val 149.394 test 147.931 (88.9 sec)\n",
            "> epoch 006: train 144.612 val 144.822 test 142.887 (88.7 sec)\n",
            "> epoch 007: train 136.459 val 122.410 test 123.573 (88.5 sec)\n",
            "> epoch 008: train 129.189 val 129.402 test 128.389 (88.9 sec)\n",
            "> epoch 009: train 122.464 val 118.462 test 118.207 (88.5 sec)\n",
            "> epoch 010: train 116.397 val 108.384 test 107.435 (88.2 sec)\n",
            "> epoch 011: train 111.147 val 103.638 test 103.466 (88.8 sec)\n",
            "> epoch 012: train 107.639 val 100.743 test 100.658 (88.9 sec)\n",
            "> epoch 013: train 104.670 val 95.762 test 96.242 (89.3 sec)\n",
            "> epoch 014: train 100.548 val 91.055 test 90.625 (88.3 sec)\n",
            "> epoch 015: train 97.609 val 90.724 test 91.185 (88.2 sec)\n",
            "> epoch 016: train 95.725 val 89.440 test 89.129 (88.5 sec)\n",
            "> epoch 017: train 92.680 val 97.286 test 96.867 (88.3 sec)\n",
            "> epoch 018: train 91.150 val 87.705 test 87.637 (88.1 sec)\n",
            "> epoch 019: train 89.148 val 87.153 test 86.428 (88.1 sec)\n",
            "> epoch 020: train 87.275 val 84.786 test 85.164 (87.8 sec)\n",
            "> epoch 021: train 87.131 val 80.221 test 81.000 (87.7 sec)\n",
            "> epoch 022: train 84.344 val 85.565 test 84.779 (87.6 sec)\n",
            "> epoch 023: train 83.160 val 80.365 test 79.994 (87.8 sec)\n",
            "> epoch 024: train 82.142 val 77.594 test 77.749 (87.8 sec)\n",
            "> epoch 025: train 80.966 val 72.242 test 73.432 (87.9 sec)\n",
            "> epoch 026: train 78.984 val 76.173 test 75.438 (88.2 sec)\n",
            "> epoch 027: train 78.364 val 74.778 test 74.297 (88.6 sec)\n",
            "> epoch 028: train 77.646 val 81.952 test 82.314 (88.3 sec)\n",
            "> epoch 029: train 78.360 val 78.057 test 78.691 (88.2 sec)\n",
            "> epoch 030: train 76.112 val 86.840 test 86.007 (88.2 sec)\n",
            "> epoch 031: train 74.554 val 67.243 test 67.853 (88.1 sec)\n",
            "> epoch 032: train 74.336 val 71.771 test 72.538 (88.1 sec)\n",
            "> epoch 033: train 73.911 val 73.422 test 74.162 (88.3 sec)\n",
            "> epoch 034: train 73.197 val 72.280 test 73.328 (88.3 sec)\n",
            "> epoch 035: train 72.551 val 67.011 test 67.625 (88.0 sec)\n",
            "> epoch 036: train 71.788 val 73.883 test 73.959 (87.7 sec)\n",
            "> epoch 037: train 70.695 val 66.540 test 67.187 (87.8 sec)\n",
            "> epoch 038: train 70.350 val 68.338 test 68.695 (87.8 sec)\n",
            "> epoch 039: train 70.049 val 68.133 test 68.051 (87.7 sec)\n",
            "> epoch 040: train 68.793 val 68.946 test 68.249 (88.1 sec)\n",
            "> epoch 041: train 68.775 val 74.232 test 74.844 (87.9 sec)\n",
            "> epoch 042: train 68.316 val 61.974 test 62.483 (88.2 sec)\n",
            "> epoch 043: train 67.757 val 65.339 test 65.660 (88.1 sec)\n",
            "> epoch 044: train 67.082 val 62.731 test 63.249 (88.3 sec)\n",
            "> epoch 045: train 66.211 val 63.795 test 64.395 (88.0 sec)\n",
            "> epoch 046: train 66.428 val 64.190 test 63.517 (88.0 sec)\n",
            "> epoch 047: train 65.567 val 63.536 test 63.349 (88.1 sec)\n",
            "> epoch 048: train 65.479 val 63.988 test 63.950 (87.9 sec)\n",
            "> epoch 049: train 65.137 val 61.350 test 61.410 (88.0 sec)\n",
            "> epoch 050: train 64.485 val 61.653 test 62.016 (87.8 sec)\n",
            "> epoch 051: train 67.664 val 70.454 test 69.950 (87.8 sec)\n",
            "> epoch 052: train 64.036 val 64.126 test 65.164 (87.8 sec)\n",
            "> epoch 053: train 64.349 val 68.855 test 69.240 (87.8 sec)\n",
            "> epoch 054: train 63.786 val 58.539 test 58.131 (87.9 sec)\n",
            "> epoch 055: train 64.471 val 62.355 test 62.806 (88.5 sec)\n",
            "> epoch 056: train 62.236 val 59.036 test 58.210 (88.1 sec)\n",
            "> epoch 057: train 61.758 val 68.995 test 69.589 (88.3 sec)\n",
            "> epoch 058: train 61.821 val 60.229 test 59.848 (88.4 sec)\n",
            "> epoch 059: train 61.504 val 60.552 test 60.406 (87.9 sec)\n",
            "> epoch 060: train 60.900 val 60.334 test 60.801 (87.8 sec)\n",
            "> epoch 061: train 60.837 val 63.062 test 63.830 (88.1 sec)\n",
            "> epoch 062: train 60.595 val 64.259 test 64.697 (87.9 sec)\n",
            "> epoch 063: train 60.341 val 62.279 test 61.937 (88.1 sec)\n",
            "> epoch 064: train 59.697 val 60.291 test 59.081 (87.7 sec)\n",
            "> epoch 065: train 59.704 val 56.878 test 56.864 (87.7 sec)\n",
            "> epoch 066: train 59.349 val 58.565 test 59.139 (88.4 sec)\n",
            "> epoch 067: train 59.337 val 58.374 test 58.632 (88.4 sec)\n",
            "> epoch 068: train 58.715 val 56.765 test 57.717 (88.3 sec)\n",
            "> epoch 069: train 58.053 val 59.773 test 59.381 (88.8 sec)\n",
            "> epoch 070: train 58.351 val 62.466 test 63.404 (88.6 sec)\n",
            "> epoch 071: train 57.811 val 57.229 test 57.222 (88.4 sec)\n",
            "> epoch 072: train 57.371 val 56.980 test 57.250 (88.8 sec)\n",
            "> epoch 073: train 57.325 val 57.534 test 57.204 (88.4 sec)\n",
            "> epoch 074: train 57.120 val 57.321 test 57.710 (87.9 sec)\n",
            "> epoch 075: train 56.210 val 60.165 test 61.014 (88.2 sec)\n",
            "> epoch 076: train 57.105 val 56.912 test 56.128 (88.1 sec)\n",
            "> epoch 077: train 56.434 val 57.515 test 57.722 (88.2 sec)\n",
            "> epoch 078: train 55.925 val 57.884 test 58.031 (87.8 sec)\n",
            "> epoch 079: train 55.681 val 54.419 test 54.751 (87.7 sec)\n",
            "> epoch 080: train 55.724 val 54.943 test 55.925 (87.4 sec)\n",
            "> epoch 081: train 54.937 val 56.530 test 56.403 (87.2 sec)\n",
            "> epoch 082: train 55.573 val 53.640 test 54.329 (87.2 sec)\n",
            "> epoch 083: train 55.408 val 57.738 test 57.521 (87.2 sec)\n",
            "> epoch 084: train 54.710 val 54.740 test 55.245 (87.1 sec)\n",
            "> epoch 085: train 54.420 val 55.174 test 55.339 (87.4 sec)\n",
            "> epoch 086: train 54.169 val 56.655 test 56.754 (87.5 sec)\n",
            "> epoch 087: train 53.593 val 59.580 test 60.095 (87.5 sec)\n",
            "> epoch 088: train 53.894 val 58.240 test 57.445 (87.5 sec)\n",
            "> epoch 089: train 54.016 val 54.713 test 54.465 (87.5 sec)\n",
            "> epoch 090: train 53.535 val 55.171 test 55.172 (87.7 sec)\n",
            "> epoch 091: train 53.274 val 53.736 test 54.477 (87.7 sec)\n",
            "> epoch 092: train 52.864 val 53.844 test 54.401 (87.5 sec)\n",
            "> epoch 093: train 53.290 val 53.449 test 53.681 (87.7 sec)\n",
            "> epoch 094: train 52.575 val 52.223 test 52.727 (87.6 sec)\n",
            "> epoch 095: train 52.329 val 53.038 test 52.634 (87.3 sec)\n",
            "> epoch 096: train 51.956 val 54.806 test 55.484 (87.6 sec)\n",
            "> epoch 097: train 52.717 val 52.421 test 52.084 (87.6 sec)\n",
            "> epoch 098: train 51.561 val 56.779 test 55.840 (87.2 sec)\n",
            "> epoch 099: train 51.572 val 54.109 test 55.090 (87.2 sec)\n",
            "> epoch 100: train 51.390 val 57.507 test 58.222 (87.1 sec)\n",
            "> epoch 101: train 51.896 val 53.646 test 53.918 (87.1 sec)\n",
            "> epoch 102: train 51.034 val 57.654 test 58.584 (87.0 sec)\n",
            "> epoch 103: train 50.843 val 52.331 test 52.476 (87.1 sec)\n",
            "> epoch 104: train 51.052 val 54.064 test 54.139 (87.5 sec)\n",
            "> epoch 105: train 50.934 val 51.296 test 51.317 (87.4 sec)\n",
            "> epoch 106: train 50.616 val 52.270 test 52.186 (87.6 sec)\n",
            "> epoch 107: train 50.264 val 51.858 test 52.009 (87.7 sec)\n",
            "> epoch 108: train 50.375 val 51.190 test 51.862 (87.5 sec)\n",
            "> epoch 109: train 50.169 val 53.475 test 53.101 (87.2 sec)\n",
            "> epoch 110: train 49.851 val 51.875 test 52.659 (87.4 sec)\n",
            "> epoch 111: train 49.730 val 52.219 test 52.677 (87.4 sec)\n",
            "> epoch 112: train 49.921 val 52.140 test 52.240 (87.2 sec)\n",
            "> epoch 113: train 49.361 val 52.714 test 53.368 (87.2 sec)\n",
            "> epoch 114: train 49.415 val 51.271 test 52.177 (87.0 sec)\n",
            "> epoch 115: train 49.628 val 51.237 test 51.707 (87.0 sec)\n",
            "> epoch 116: train 49.283 val 52.038 test 53.109 (86.9 sec)\n",
            "> epoch 117: train 50.809 val 55.883 test 55.545 (86.8 sec)\n",
            "> epoch 118: train 48.712 val 52.393 test 52.454 (87.5 sec)\n",
            "> epoch 119: train 49.085 val 50.369 test 50.547 (88.0 sec)\n",
            "> epoch 120: train 48.481 val 55.204 test 55.358 (87.4 sec)\n",
            "> epoch 121: train 49.095 val 49.267 test 49.794 (87.8 sec)\n",
            "> epoch 122: train 48.918 val 50.106 test 50.582 (87.8 sec)\n",
            "> epoch 123: train 48.587 val 52.362 test 53.178 (87.8 sec)\n",
            "> epoch 124: train 48.560 val 52.890 test 53.689 (87.9 sec)\n",
            "> epoch 125: train 48.394 val 51.341 test 51.180 (88.0 sec)\n",
            "> epoch 126: train 48.264 val 53.787 test 54.752 (87.9 sec)\n",
            "> epoch 127: train 47.837 val 52.838 test 53.060 (88.1 sec)\n",
            "> epoch 128: train 48.004 val 51.343 test 51.510 (87.6 sec)\n",
            "> epoch 129: train 47.471 val 51.570 test 52.210 (88.1 sec)\n",
            "> epoch 130: train 47.703 val 50.827 test 51.244 (87.5 sec)\n",
            "> epoch 131: train 47.745 val 49.974 test 50.543 (87.8 sec)\n",
            "> epoch 132: "
          ]
        }
      ],
      "source": [
        "print('> start training')\n",
        " \n",
        "\n",
        "tr_ys = train_loader.dataset.data['homo'] \n",
        "me, mad = torch.mean(tr_ys), torch.mean(torch.abs(tr_ys - torch.mean(tr_ys)))\n",
        "\n",
        "if cuda:\n",
        "    me = me.cuda()\n",
        "    mad = mad.cuda()\n",
        "\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "test_loss = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('> epoch %s:' % str(epoch).zfill(3), end=' ', flush=True) \n",
        "    start = time.time()\n",
        "\n",
        "    batch_train_loss = []\n",
        "    batch_val_loss = []\n",
        "    batch_test_loss = []\n",
        "\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        batch = BatchGraph(batch, cuda, charge_scale)\n",
        "        \n",
        "        out = model(batch).reshape(-1)\n",
        "        # compute l1-loss \n",
        "        loss =  F.l1_loss(out,  (batch.y - me) / mad)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            loss =  F.l1_loss(out * mad + me, batch.y)\n",
        "\n",
        "        batch_train_loss += [float(loss.data.cpu().numpy())]  \n",
        "        \n",
        "    train_loss += [np.mean(batch_train_loss) / 0.001]\n",
        "    \n",
        "    print('train %.3f' % train_loss[-1], end=' ', flush=True)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for batch in val_loader:\n",
        "            batch = BatchGraph(batch, cuda, charge_scale)\n",
        "            out = model(batch).reshape(-1)\n",
        "            loss = F.l1_loss(out * mad + me, batch.y).data.cpu().numpy()\n",
        "            batch_val_loss += [np.mean(loss)]\n",
        "            \n",
        "        val_loss += [np.mean(batch_val_loss) / 0.001]\n",
        "        \n",
        "        print('val %.3f' % val_loss[-1], end=' ', flush=True)\n",
        "        \n",
        "        for batch in test_loader:\n",
        "            batch = BatchGraph(batch, cuda, charge_scale)\n",
        "            out = model(batch).reshape(-1)\n",
        "            loss = F.l1_loss(out * mad + me, batch.y).data.cpu().numpy()\n",
        "            batch_test_loss += [np.mean(loss)]\n",
        "\n",
        "        test_loss += [np.mean(batch_test_loss) / 0.001]\n",
        "        \n",
        "    end = time.time()\n",
        "\n",
        "    print('test %.3f (%.1f sec)' % (test_loss[-1], end-start), flush=True)\n",
        "    lr_scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RfUzRc-PjTIc"
      },
      "id": "RfUzRc-PjTIc",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Final_Graph.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}